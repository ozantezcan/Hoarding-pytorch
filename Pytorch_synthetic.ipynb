{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/torchsample-0.1.2-py3.6.egg/torchsample/datasets.py:16: UserWarning: Cant import nibabel.. Cant load brain images\n"
     ]
    }
   ],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "768\n",
      "329\n",
      "329\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training \n",
    "# Just normalization for validation\n",
    "dataset_both=False\n",
    "dataset='real'\n",
    "uniform_sampler=False\n",
    "batch_size=32\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        #ts_transforms.RandomRotate(30)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "if(dataset=='real'):\n",
    "    data_dir = '..//Data_Sets//pruned//good'\n",
    "    dsets = {x: datasets.ImageFolder_mtezcan([os.path.join(data_dir, x)], data_transforms[x])\n",
    "             for x in ['train', 'val']}\n",
    "    dsets_real = dsets\n",
    "if(dataset=='synthetic'):\n",
    "    rootdir='//media//mtezcan//New Volume/HoardingImages//_rated//'\n",
    "    valdir='//media//mtezcan//New Volume/HoardingImages//_val//validation//House//BR//'\n",
    "    #subdirs=os.listdir(rootdir)\n",
    "    subdirs = ['BasicHouse_2017-07-01-rated',\n",
    "               'BriansHouse_2017-06-30-rated',\n",
    "               'RuralHome_2017-06-30-rated',\n",
    "               'SmallApt_2017-06-29-rated']\n",
    "    roomdirs=['//BR','//Kitchen','//LR']\n",
    "    '''\n",
    "    rootdir='//media//mtezcan//New Volume/HoardingImages//_rated//train//'\n",
    "    valdir='//media//mtezcan//New Volume/HoardingImages//_rated//val//BriansHouse_2017-06-30-rated//LR//'\n",
    "    subdirs=['BriansHouse_2017-06-30-rated']\n",
    "    print(subdirs)\n",
    "    roomdirs=['//LR']\n",
    "    '''\n",
    "    dsets = {'train':datasets.ImageFolder_mtezcan([rootdir+subdir+room for subdir in subdirs\n",
    "                                                   for room in roomdirs], data_transforms['train']),\n",
    "             'val':datasets.ImageFolder_mtezcan([valdir], data_transforms['val'])}\n",
    " \n",
    "if(uniform_sampler):\n",
    "    weights,wpc = ft.make_weights_for_balanced_classes(dsets['train'].imgs, len(dsets['train'].classes))  \n",
    "    weights = torch.DoubleTensor(weights) \n",
    "    sampler = {'train':torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) ,\n",
    "               'val':None}\n",
    "else:\n",
    "    sampler = {'train':None,\n",
    "               'val':None}\n",
    "\n",
    "shuffler={'train':True,'val':False}\n",
    "dset_loaders = {x:torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,shuffle=shuffler[x],sampler=sampler[x], num_workers=12)\n",
    "                for x in ['train','val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['train'].classes\n",
    "print(dset_classes)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if(dataset=='real'):\n",
    "    dset_loaders_real=dset_loaders\n",
    "    dset_sizes_real=dset_sizes\n",
    "elif(dataset=='synthetic'):\n",
    "    dset_loaders_synthetic=dset_loaders\n",
    "    dset_sizes_synthetic=dset_sizes\n",
    "#use_gpu=False\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "Let's visualize a few training images so as to understand the data\n",
    "augmentations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#inputs, classes = next(iter(dset_loaders_real['train']))\n",
    "#print(classes.cpu().numpy().reshape(4,4)+1)\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs,nrow=8)\n",
    "print('Size of the input tensors in one batch after grid is  '+str(out.size()))\n",
    "plt.figure(figsize=(12,12))\n",
    "ft.imshow(out, title=[dset_classes[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the convnet\n",
    "----------------------\n",
    "\n",
    "Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    f=open('runs/'+logname+'/Network_properties.txt','w')\n",
    "    if(dataset_both):\n",
    "        f.write('Dataset: both\\n')\n",
    "    else:\n",
    "        f.write('Dataset: '+dataset+'\\n')\n",
    "    f.write('Network: '+network+'\\n')\n",
    "    f.write('Uniform sampler: '+str(uniform_sampler)+'\\n')\n",
    "    f.write('Criterion: '+criteria+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.write('Network is pretrained: '+str(pretrained)+'\\n')\n",
    "    f.write('Network laoded from: '+networkName+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network='loaded'\n",
    "networkName='resnet18_synthetic_sgd_multisoft_July22  16:05:30'\n",
    "optimizer='sgd'\n",
    "criteria='multisoft'\n",
    "end_to_end=True\n",
    "lr=0.01\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=None#ft.exp_lr_scheduler\n",
    "lr_decay_epoch=10\n",
    "pretrained=True\n",
    "\n",
    "if(network=='resnet18'):\n",
    "    model_ft = models.resnet18(pretrained=pretrained)\n",
    "    if not end_to_end:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False \n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 9)\n",
    "elif(network=='alexnet'):\n",
    "    model_ft = models.alexnet(pretrained=pretrained)\n",
    "    num_ftrs = model_ft.classifier[6].out_features\n",
    "    setattr(model_ft.classifier, '7', nn.ReLU(inplace=True))\n",
    "    setattr(model_ft.classifier, '8', nn.Dropout())\n",
    "    setattr(model_ft.classifier, '9', nn.Linear(num_ftrs,9))\n",
    "elif(network=='loaded'):\n",
    "    model_ft = torch.load('./saved_models/'+networkName)\n",
    "    if not end_to_end:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False \n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 9)\n",
    "else:\n",
    "    raise ValueError('Undefined network '+network)\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "if(criteria=='crossentropy'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif(criteria=='multisoft'):\n",
    "    criterion=nn.MultiLabelSoftMarginLoss()\n",
    "else:\n",
    "    raise ValueError('Undefined criteria '+criteria)\n",
    "    \n",
    "if(optimizer=='adam'):\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "elif(optimizer=='sgd'):\n",
    "    if(end_to_end):\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.0188 Acc: 0.1849 CIR-1: 0.4544\n",
      "val Loss: 0.0183 Acc: 0.2067 CIR-1: 0.5684\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0168 Acc: 0.2227 CIR-1: 0.5599\n",
      "val Loss: 0.0168 Acc: 0.2249 CIR-1: 0.6140\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0157 Acc: 0.2409 CIR-1: 0.6328\n",
      "val Loss: 0.0167 Acc: 0.2432 CIR-1: 0.6474\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.2526 CIR-1: 0.6536\n",
      "val Loss: 0.0164 Acc: 0.2705 CIR-1: 0.6444\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0153 Acc: 0.2747 CIR-1: 0.6419\n",
      "val Loss: 0.0177 Acc: 0.2310 CIR-1: 0.6109\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0148 Acc: 0.2552 CIR-1: 0.6628\n",
      "val Loss: 0.0162 Acc: 0.2614 CIR-1: 0.6657\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0145 Acc: 0.2786 CIR-1: 0.7031\n",
      "val Loss: 0.0179 Acc: 0.1976 CIR-1: 0.6170\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0145 Acc: 0.2526 CIR-1: 0.6940\n",
      "val Loss: 0.0165 Acc: 0.2584 CIR-1: 0.6717\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0141 Acc: 0.2773 CIR-1: 0.7018\n",
      "val Loss: 0.0209 Acc: 0.1976 CIR-1: 0.5319\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0140 Acc: 0.2630 CIR-1: 0.7148\n",
      "val Loss: 0.0177 Acc: 0.2158 CIR-1: 0.6535\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0139 Acc: 0.2760 CIR-1: 0.7201\n",
      "val Loss: 0.0154 Acc: 0.2796 CIR-1: 0.7204\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0136 Acc: 0.2891 CIR-1: 0.7344\n",
      "val Loss: 0.0154 Acc: 0.2614 CIR-1: 0.7052\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0133 Acc: 0.3086 CIR-1: 0.7513\n",
      "val Loss: 0.0151 Acc: 0.2675 CIR-1: 0.7112\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0130 Acc: 0.2917 CIR-1: 0.7539\n",
      "val Loss: 0.0160 Acc: 0.3100 CIR-1: 0.7386\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0130 Acc: 0.3177 CIR-1: 0.7786\n",
      "val Loss: 0.0193 Acc: 0.2097 CIR-1: 0.6170\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0131 Acc: 0.2721 CIR-1: 0.7565\n",
      "val Loss: 0.0161 Acc: 0.2979 CIR-1: 0.6960\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0127 Acc: 0.3034 CIR-1: 0.7591\n",
      "val Loss: 0.0220 Acc: 0.2067 CIR-1: 0.5471\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.3125 CIR-1: 0.7917\n",
      "val Loss: 0.0178 Acc: 0.2219 CIR-1: 0.6565\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0127 Acc: 0.2904 CIR-1: 0.7643\n",
      "val Loss: 0.0154 Acc: 0.3009 CIR-1: 0.7234\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.3086 CIR-1: 0.7695\n",
      "val Loss: 0.0157 Acc: 0.2948 CIR-1: 0.7325\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0129 Acc: 0.2930 CIR-1: 0.7826\n",
      "val Loss: 0.0153 Acc: 0.3222 CIR-1: 0.7599\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.3034 CIR-1: 0.7878\n",
      "val Loss: 0.0149 Acc: 0.3191 CIR-1: 0.7842\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0118 Acc: 0.3333 CIR-1: 0.8086\n",
      "val Loss: 0.0171 Acc: 0.2614 CIR-1: 0.6596\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0119 Acc: 0.3047 CIR-1: 0.8008\n",
      "val Loss: 0.0238 Acc: 0.1702 CIR-1: 0.5380\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0120 Acc: 0.3125 CIR-1: 0.8190\n",
      "val Loss: 0.0162 Acc: 0.2736 CIR-1: 0.6869\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0116 Acc: 0.3346 CIR-1: 0.8242\n",
      "val Loss: 0.0186 Acc: 0.2371 CIR-1: 0.6322\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0118 Acc: 0.3307 CIR-1: 0.8151\n",
      "val Loss: 0.0152 Acc: 0.2888 CIR-1: 0.7447\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0113 Acc: 0.3281 CIR-1: 0.8294\n",
      "val Loss: 0.0163 Acc: 0.2705 CIR-1: 0.7204\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.3438 CIR-1: 0.8333\n",
      "val Loss: 0.0150 Acc: 0.2948 CIR-1: 0.7508\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.2982 CIR-1: 0.8346\n",
      "val Loss: 0.0160 Acc: 0.3131 CIR-1: 0.7295\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.3438 CIR-1: 0.8503\n",
      "val Loss: 0.0161 Acc: 0.2705 CIR-1: 0.7204\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.3581 CIR-1: 0.8307\n",
      "val Loss: 0.0148 Acc: 0.2553 CIR-1: 0.7173\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.3477 CIR-1: 0.8281\n",
      "val Loss: 0.0184 Acc: 0.3009 CIR-1: 0.6991\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0116 Acc: 0.3464 CIR-1: 0.8255\n",
      "val Loss: 0.0169 Acc: 0.3040 CIR-1: 0.7234\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.3607 CIR-1: 0.8581\n",
      "val Loss: 0.0171 Acc: 0.2705 CIR-1: 0.7204\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.3711 CIR-1: 0.8620\n",
      "val Loss: 0.0161 Acc: 0.2492 CIR-1: 0.7021\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0101 Acc: 0.3971 CIR-1: 0.8698\n",
      "val Loss: 0.0182 Acc: 0.2705 CIR-1: 0.6991\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0101 Acc: 0.3828 CIR-1: 0.8685\n",
      "val Loss: 0.0187 Acc: 0.2644 CIR-1: 0.6839\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0101 Acc: 0.3711 CIR-1: 0.8685\n",
      "val Loss: 0.0162 Acc: 0.2796 CIR-1: 0.7386\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0100 Acc: 0.3477 CIR-1: 0.8568\n",
      "val Loss: 0.0167 Acc: 0.2857 CIR-1: 0.7447\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0100 Acc: 0.3698 CIR-1: 0.8646\n",
      "val Loss: 0.0178 Acc: 0.2736 CIR-1: 0.6809\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.4010 CIR-1: 0.8789\n",
      "val Loss: 0.0188 Acc: 0.2675 CIR-1: 0.6869\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.4414 CIR-1: 0.9049\n",
      "val Loss: 0.0190 Acc: 0.2280 CIR-1: 0.6748\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.4115 CIR-1: 0.9010\n",
      "val Loss: 0.0165 Acc: 0.2948 CIR-1: 0.7690\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.4154 CIR-1: 0.9049\n",
      "val Loss: 0.0181 Acc: 0.2888 CIR-1: 0.7173\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0094 Acc: 0.4036 CIR-1: 0.8854\n",
      "val Loss: 0.0181 Acc: 0.2796 CIR-1: 0.6930\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.3984 CIR-1: 0.8945\n",
      "val Loss: 0.0162 Acc: 0.3070 CIR-1: 0.7751\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.4297 CIR-1: 0.9089\n",
      "val Loss: 0.0172 Acc: 0.3009 CIR-1: 0.7264\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0094 Acc: 0.4089 CIR-1: 0.8867\n",
      "val Loss: 0.0202 Acc: 0.2432 CIR-1: 0.6505\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0089 Acc: 0.4258 CIR-1: 0.9049\n",
      "val Loss: 0.0175 Acc: 0.2553 CIR-1: 0.7508\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.4128 CIR-1: 0.8958\n",
      "val Loss: 0.0255 Acc: 0.2432 CIR-1: 0.6474\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0085 Acc: 0.4492 CIR-1: 0.9167\n",
      "val Loss: 0.0219 Acc: 0.2462 CIR-1: 0.6748\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.4245 CIR-1: 0.9049\n",
      "val Loss: 0.0176 Acc: 0.2888 CIR-1: 0.7477\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.4232 CIR-1: 0.9154\n",
      "val Loss: 0.0186 Acc: 0.2705 CIR-1: 0.6930\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.4648 CIR-1: 0.9427\n",
      "val Loss: 0.0182 Acc: 0.2705 CIR-1: 0.7356\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.4714 CIR-1: 0.9258\n",
      "val Loss: 0.0183 Acc: 0.3070 CIR-1: 0.7386\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0082 Acc: 0.4570 CIR-1: 0.9076\n",
      "val Loss: 0.0202 Acc: 0.2857 CIR-1: 0.6960\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0080 Acc: 0.4466 CIR-1: 0.9180\n",
      "val Loss: 0.0203 Acc: 0.2584 CIR-1: 0.6717\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.4714 CIR-1: 0.9245\n",
      "val Loss: 0.0232 Acc: 0.2827 CIR-1: 0.6900\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.4805 CIR-1: 0.9310\n",
      "val Loss: 0.0209 Acc: 0.2462 CIR-1: 0.6778\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0081 Acc: 0.4518 CIR-1: 0.9284\n",
      "val Loss: 0.0205 Acc: 0.2948 CIR-1: 0.7112\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.4857 CIR-1: 0.9336\n",
      "val Loss: 0.0185 Acc: 0.2796 CIR-1: 0.7447\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.5091 CIR-1: 0.9518\n",
      "val Loss: 0.0181 Acc: 0.2827 CIR-1: 0.7416\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.5052 CIR-1: 0.9401\n",
      "val Loss: 0.0189 Acc: 0.2675 CIR-1: 0.7173\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.4987 CIR-1: 0.9453\n",
      "val Loss: 0.0186 Acc: 0.3040 CIR-1: 0.7720\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.5391 CIR-1: 0.9388\n",
      "val Loss: 0.0204 Acc: 0.2736 CIR-1: 0.7264\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.5117 CIR-1: 0.9505\n",
      "val Loss: 0.0218 Acc: 0.2523 CIR-1: 0.6960\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.5417 CIR-1: 0.9518\n",
      "val Loss: 0.0214 Acc: 0.3040 CIR-1: 0.7204\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.5260 CIR-1: 0.9544\n",
      "val Loss: 0.0265 Acc: 0.2614 CIR-1: 0.6505\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.5000 CIR-1: 0.9323\n",
      "val Loss: 0.0229 Acc: 0.2614 CIR-1: 0.6565\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.5391 CIR-1: 0.9258\n",
      "val Loss: 0.0206 Acc: 0.2553 CIR-1: 0.6991\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.5013 CIR-1: 0.9466\n",
      "val Loss: 0.0204 Acc: 0.2796 CIR-1: 0.7234\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0064 Acc: 0.5286 CIR-1: 0.9544\n",
      "val Loss: 0.0211 Acc: 0.2979 CIR-1: 0.7143\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.5195 CIR-1: 0.9596\n",
      "val Loss: 0.0213 Acc: 0.2492 CIR-1: 0.7143\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.5156 CIR-1: 0.9531\n",
      "val Loss: 0.0270 Acc: 0.3040 CIR-1: 0.6930\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.5417 CIR-1: 0.9648\n",
      "val Loss: 0.0341 Acc: 0.1976 CIR-1: 0.6049\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0060 Acc: 0.5469 CIR-1: 0.9557\n",
      "val Loss: 0.0243 Acc: 0.2705 CIR-1: 0.6839\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.5586 CIR-1: 0.9688\n",
      "val Loss: 0.0200 Acc: 0.2796 CIR-1: 0.6900\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0060 Acc: 0.5586 CIR-1: 0.9583\n",
      "val Loss: 0.0244 Acc: 0.2736 CIR-1: 0.6809\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.5755 CIR-1: 0.9688\n",
      "val Loss: 0.0206 Acc: 0.2584 CIR-1: 0.7234\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0061 Acc: 0.5977 CIR-1: 0.9609\n",
      "val Loss: 0.0200 Acc: 0.3283 CIR-1: 0.7295\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.5378 CIR-1: 0.9648\n",
      "val Loss: 0.0215 Acc: 0.2827 CIR-1: 0.6991\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.5872 CIR-1: 0.9701\n",
      "val Loss: 0.0208 Acc: 0.2888 CIR-1: 0.7508\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.5820 CIR-1: 0.9766\n",
      "val Loss: 0.0199 Acc: 0.3161 CIR-1: 0.7751\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.5690 CIR-1: 0.9714\n",
      "val Loss: 0.0229 Acc: 0.2736 CIR-1: 0.7112\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.5573 CIR-1: 0.9544\n",
      "val Loss: 0.0258 Acc: 0.2827 CIR-1: 0.6900\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.5938 CIR-1: 0.9753\n",
      "val Loss: 0.0214 Acc: 0.3009 CIR-1: 0.7143\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.5547 CIR-1: 0.9674\n",
      "val Loss: 0.0210 Acc: 0.3070 CIR-1: 0.6991\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.5260 CIR-1: 0.9674\n",
      "val Loss: 0.0201 Acc: 0.2705 CIR-1: 0.7386\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.5677 CIR-1: 0.9701\n",
      "val Loss: 0.0207 Acc: 0.2888 CIR-1: 0.7386\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.5690 CIR-1: 0.9818\n",
      "val Loss: 0.0216 Acc: 0.2857 CIR-1: 0.7386\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.5625 CIR-1: 0.9622\n",
      "val Loss: 0.0208 Acc: 0.3070 CIR-1: 0.7112\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.6198 CIR-1: 0.9753\n",
      "val Loss: 0.0218 Acc: 0.2857 CIR-1: 0.7112\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.5911 CIR-1: 0.9740\n",
      "val Loss: 0.0201 Acc: 0.2979 CIR-1: 0.7477\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.6250 CIR-1: 0.9766\n",
      "val Loss: 0.0202 Acc: 0.2918 CIR-1: 0.7173\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.6068 CIR-1: 0.9792\n",
      "val Loss: 0.0263 Acc: 0.2553 CIR-1: 0.6748\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.5664 CIR-1: 0.9779\n",
      "val Loss: 0.0218 Acc: 0.2948 CIR-1: 0.7295\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.6250 CIR-1: 0.9818\n",
      "val Loss: 0.0245 Acc: 0.2614 CIR-1: 0.6991\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.5977 CIR-1: 0.9779\n",
      "val Loss: 0.0246 Acc: 0.2888 CIR-1: 0.6444\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.5794 CIR-1: 0.9818\n",
      "val Loss: 0.0234 Acc: 0.2492 CIR-1: 0.6960\n",
      "\n",
      "Training complete in 18m 48s\n",
      "Best val Acc: 0.328267\n"
     ]
    }
   ],
   "source": [
    "if(dataset_both):\n",
    "    data_name='both'\n",
    "else:\n",
    "    data_name=dataset\n",
    "    \n",
    "logname=network+'_'+data_name+'_'+optimizer+'_'+criteria+'_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "writer = SummaryWriter('runs/'+logname)\n",
    "writeLog(logname)\n",
    "if(dataset_both):\n",
    "    model_ft = ft.train_model_both(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders_real,\n",
    "                         dset_sizes_real, dset_loaders_synthetic, dset_sizes_synthetic,writer,use_gpu=use_gpu,\n",
    "                            num_epochs=100,batch_size=batch_size,num_train=2,num_test=2,multilabel=True,\n",
    "                              multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr)\n",
    "elif(uniform_sampler):\n",
    "    model_ft = ft.train_model_balanced(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                            use_gpu=use_gpu,num_epochs=100,batch_size=batch_size,num_train=10,num_test=2,\n",
    "                                multilabel=True, multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr)\n",
    "else:\n",
    "    model_ft = ft.train_model(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                        use_gpu=use_gpu,num_epochs=100,batch_size=batch_size,num_log=200,multilabel=True,\n",
    "                          multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft,'./saved_models/'+logname)\n",
    "model_ft_backup=model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_ft=torch.load('./Obsolete/saved_models/resnet18_multi_88_real_7_15_17.mdl')\n",
    "model_ft=torch.load('./saved_models/resnet_real_ft5s_multisoft__1300_100epoch_July19  12:22:17')\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion=nn.MultiLabelSoftMarginLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "model_ft = model_ft.cuda()\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# --- to-be-optimized ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(next(iter(dset_loaders['train']))[0])\n",
    "#model_ft = models.alexnet(pretrained=True)\n",
    "\n",
    "model_ft=model_ft.cpu()\n",
    "model_ft.train(False)\n",
    "#new_classifier = nn.Sequential(*list(model_ft.classifier.children())[:-5])\n",
    "#model_ft.classifier = new_classifier\n",
    "#print(model_ft)\n",
    "model_params= list(model_ft.children())\n",
    "#model_params[1]=list(model_params[1])\n",
    "#print(model_params)\n",
    "new_ft = nn.Sequential(*list(model_params)[:-1])\n",
    "#print(new_ft)\n",
    "\n",
    "fvec_tr=np.zeros((20000,512))\n",
    "label_tr=np.zeros((20000))\n",
    "\n",
    "fvec_val=np.zeros((20000,512))\n",
    "label_val=np.zeros((20000))\n",
    "count=0;\n",
    "\n",
    "#inputs_t, classes_t = data=next(iter(dset_loaders['train']))\n",
    "#print(inputs_t.size())\n",
    "#fvec_t=new_ft(Variable(inputs_t))\n",
    "\n",
    "for data in dset_loaders['train']:\n",
    "    inputs_t, classes_t = data\n",
    "    fvec_t=new_ft(Variable(inputs_t))\n",
    "    #print(fvec_t)\n",
    "    fvec_t_cpu=fvec_t.cpu()\n",
    "    if(fvec_t_cpu.data.numpy().shape[0]==4):\n",
    "        fvec_tr[count:count+4,:]=fvec_t_cpu.data.numpy().reshape(4,-1)\n",
    "        label_tr[count:count+4]=classes_t.short().numpy()\n",
    "        count +=4\n",
    "fvec_tr=fvec_tr[:count,:]\n",
    "label_tr=label_tr[:count]\n",
    "\n",
    "\n",
    "count=0;\n",
    "for data in dset_loaders['val']:\n",
    "    inputs_t, classes_t = data\n",
    "    fvec_t=new_ft(Variable(inputs_t))\n",
    "    fvec_t_cpu=fvec_t.cpu()\n",
    "    if(fvec_t_cpu.data.numpy().shape[0]==4):\n",
    "        fvec_val[count:count+4,:]=fvec_t_cpu.data.numpy().reshape(4,-1)\n",
    "        label_val[count:count+4]=classes_t.short().numpy()\n",
    "        count +=4\n",
    "    \n",
    "fvec_val=fvec_val[:count,:]\n",
    "label_val=label_val[:count]\n",
    "\n",
    "'''\n",
    "print('The CNN model is')\n",
    "print(model_ft)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "#model_ft.classifier['6'] = nn.Linear(num_ftrs, 2)\n",
    "print(model_ft)\n",
    "'''\n",
    "\n",
    "print('Size of the input in training is '+str(fvec_tr.shape))\n",
    "print('Size of the labels in training is '+str(label_tr.shape))\n",
    "\n",
    "print('Size of the input in validation is '+str(fvec_val.shape))\n",
    "print('Size of the labels in validation is '+str(label_val.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fvec_val.shape)\n",
    "print(fvec_val[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#clf = SVC(C=1,kernel='linear',class_weight='balanced',decision_function_shape='ovo')\n",
    "clf=OneVsOneClassifier(LinearSVC(C=1e-5))\n",
    "clf.fit(fvec_tr,label_tr)\n",
    "label_pred=clf.predict(fvec_val)\n",
    "abs_err=np.abs(label_pred-label_val)\n",
    "cir1=np.mean(abs_err<=1)\n",
    "print(cir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c_val=np.power(2,np.arange(0.,8.,1.))\n",
    "g_val=np.power(2,np.arange(-10.,-5.,1.))\n",
    "\n",
    "#c_val=[128]\n",
    "#g_val=[np.power(2,-10.)]\n",
    "cir1_max=0.\n",
    "c_max=0\n",
    "g_max=0\n",
    "for c_1 in c_val:\n",
    "    for g in g_val:\n",
    "        \n",
    "        clf_svc = SVC(C=c_1,kernel='rbf',gamma=g,shrinking=False,class_weight='balanced')\n",
    "        clf=OneVsOneClassifier(clf_svc)\n",
    "        #print(label_tr)\n",
    "        clf.fit(fvec_tr,label_tr)\n",
    "\n",
    "        label_pred=clf.predict(fvec_val)\n",
    "        #print(label_pred)\n",
    "        abs_err=np.abs(label_pred-label_val)\n",
    "        #print(abs_err)\n",
    "        cir1=np.mean(abs_err<=1)\n",
    "        print('C = '+str(c_1),', g = ',str(g),', cir-1 = '+str(cir1))\n",
    "        if(cir1>cir1_max):\n",
    "            cir1_max=cir1\n",
    "            c_max=c_1\n",
    "            g_max=g\n",
    "            \n",
    "\n",
    "clf = SVC(C=c_max,kernel='rbf',gamma=g_max)#,class_weight='balanced')\n",
    "clf.fit(fvec_tr,label_tr)\n",
    "label_pred=clf.predict(fvec_val)\n",
    "abs_err=np.abs(label_pred-label_val)\n",
    "print('Max CIR-1 achieved is '+str(np.mean(abs_err<=1)) +' with c='+str(c_max),', g='+str(g_max))\n",
    "print('CIR-0 = '+str(np.mean(abs_err<=0)))\n",
    "print('CIR-2 = '+str(np.mean(abs_err<=2)))\n",
    "tr_pred=clf.predict(fvec_tr)\n",
    "abs_tr=np.abs(tr_pred-label_tr)\n",
    "print('Training CIR-1 is '+str(np.mean(abs_tr<=1)))\n",
    "#print(abs_err)\n",
    "#print(label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "#print('The CNN model is')\n",
    "#print(model_ft)\n",
    "#print(list(model_ft.children())[:-1])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "#setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "model_ft.fc = nn.Linear(num_ftrs, 9)\n",
    "#print(model_ft)\n",
    "\n",
    "#print(next(iter(dset_loaders['train']))[0])\n",
    "new_ft = nn.Sequential(*list(model_ft.children())[:-1])\n",
    "#dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=4,\n",
    "#                                               shuffle=True, num_workers=4)\n",
    "#                for x in ['train', 'val']}\n",
    "\n",
    "fvec_tr=np.zeros((X_tr.shape[0],512))\n",
    "for k in range(0,10,10):#(0,inputs.shape[0],10):\n",
    "    print(k)\n",
    "    fvec_now=new_ft(Variable(torch.from_numpy(X_tr[k:k+10,:,:,:])))\n",
    "    print(fvec_now.numpy())\n",
    "\n",
    "print(fvec)\n",
    "\n",
    "\n",
    "'''\n",
    "print('The CNN model is')\n",
    "print(model_ft)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "#model_ft.classifier['6'] = nn.Linear(num_ftrs, 2)\n",
    "print(model_ft)\n",
    "'''\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet as fixed feature extractor\n",
    "----------------------------------\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set ``requires_grad == False`` to freeze the parameters so that the\n",
    "gradients are not computed in ``backward()``.\n",
    "\n",
    "You can read more about this in the documentation\n",
    "`here <http://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model_conv.fc.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 9)\n",
    "\n",
    "if use_gpu:\n",
    "    model_conv = model_conv.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_conv = ft.train_model(model_conv, criterion, optimizer_conv, ft.exp_lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                        use_gpu=use_gpu,num_epochs=100,batch_size=32,num_log=1000,multilabel=False,multi_prob=False,\n",
    "                         lr_decay_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
