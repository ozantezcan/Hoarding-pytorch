{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtezcan/anaconda3/lib/python3.6/site-packages/torchsample-0.1.2-py3.6.egg/torchsample/datasets.py:16: UserWarning: Cant import nibabel.. Cant load brain images\n"
     ]
    }
   ],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "768\n",
      "329\n",
      "329\n",
      "[12.59016393442623, 7.757575757575758, 10.816901408450704, 13.963636363636363, 8.629213483146067, 7.603960396039604, 5.605839416058394, 8.533333333333333, 11.815384615384616]\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training \n",
    "# Just normalization for validation\n",
    "dataset='real'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        #ts_transforms.RandomRotate(30)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "if(dataset=='real'):\n",
    "    data_dir = '..//Data_Sets//pruned//good'\n",
    "    dsets = {x: datasets.ImageFolder_mtezcan([os.path.join(data_dir, x)], data_transforms[x])\n",
    "             for x in ['train', 'val']}\n",
    "elif(dataset=='synthetic'):\n",
    "    rootdir='//media//mtezcan//New Volume/HoardingImages//_rated//'\n",
    "    valdir='//media//mtezcan//New Volume/HoardingImages//_val//House//BR'\n",
    "    #subdirs=os.listdir(rootdir)\n",
    "    subdirs = ['BasicHouse_2017-07-01-rated',\n",
    "               'BriansHouse_2017-06-30-rated',\n",
    "               'RuralHome_2017-06-30-rated',\n",
    "               'SmallApt_2017-06-29-rated']\n",
    "    roomdirs=['//BR','//Kitchen','//LR']\n",
    "    '''\n",
    "    rootdir='//media//mtezcan//New Volume/HoardingImages//_rated//train//'\n",
    "    valdir='//media//mtezcan//New Volume/HoardingImages//_rated//val//BriansHouse_2017-06-30-rated//LR//'\n",
    "    subdirs=['BriansHouse_2017-06-30-rated']\n",
    "    print(subdirs)\n",
    "    roomdirs=['//LR']\n",
    "    '''\n",
    "    dsets = {'train':datasets.ImageFolder_mtezcan([rootdir+subdir+room for subdir in subdirs\n",
    "                                                   for room in roomdirs], data_transforms['train']),\n",
    "             'val':datasets.ImageFolder_mtezcan([valdir], data_transforms['val'])}\n",
    "else:\n",
    "    raise ValueError('Undefined dataset '+ dataset)\n",
    " \n",
    "weights,wpc = ft.make_weights_for_balanced_classes(dsets['train'].imgs, len(dsets['train'].classes))  \n",
    "weights = torch.DoubleTensor(weights) \n",
    "#sampler = {'train':torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) ,\n",
    "sampler = {'train':None,\n",
    "           'val':None}\n",
    "shuffler={'train':True,'val':False}\n",
    "batch_size=64\n",
    "dset_loaders = {x:torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,shuffle=shuffler[x],sampler=sampler[x], num_workers=12)\n",
    "                for x in ['train','val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['train'].classes\n",
    "print(dset_classes)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "#use_gpu=False\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "Let's visualize a few training images so as to understand the data\n",
    "augmentations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "inputs, classes = next(iter(dset_loaders['train']))\n",
    "#print(classes.cpu().numpy().reshape(4,4)+1)\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs,nrow=4)\n",
    "print('Size of the input tensors in one batch after grid is  '+str(out.size()))\n",
    "plt.figure(figsize=(12,12))\n",
    "ft.imshow(out, title=[dset_classes[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the convnet\n",
    "----------------------\n",
    "\n",
    "Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    f=open('runs/logname/Network_properties.txt','w')\n",
    "    f.write('Dataset: '+dataset+'\\n')\n",
    "    f.write('Network: '+network+'\\n')\n",
    "    f.write('Criterion: '+criteria+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler is : '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period is: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network='resnet18'\n",
    "optimizer='sgd'\n",
    "criteria='multisoft'\n",
    "lr=0.01\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=None\n",
    "lr_decay_epoch=12\n",
    "\n",
    "if(network=='resnet18'):\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 9)\n",
    "elif(network=='alexnet'):\n",
    "    model_ft = models.alexnet(pretrained=True)\n",
    "    num_ftrs = model_ft.classifier[6].out_features\n",
    "    setattr(model_ft.classifier, '7', nn.ReLU(inplace=True))\n",
    "    setattr(model_ft.classifier, '8', nn.Dropout())\n",
    "    setattr(model_ft.classifier, '9', nn.Linear(num_ftrs,9))\n",
    "else:\n",
    "    raise ValueError('Undefined network '+network)\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "if(criteria=='crossentropy'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif(criteria=='multisoft'):\n",
    "    criterion=nn.MultiLabelSoftMarginLoss()\n",
    "else:\n",
    "    raise ValueError('Undefined criteria '+criteria)\n",
    "    \n",
    "if(optimizer=='adam'):\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "elif(optimizer=='sgd'):\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/logname/Network_properties.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-089786b12e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%B%d  %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runs/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwriteLog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m model_ft = ft.train_model(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders,dset_sizes,writer,\n\u001b[1;32m      5\u001b[0m                         \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmultilabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-58eee8c9f9e7>\u001b[0m in \u001b[0;36mwriteLog\u001b[0;34m(logname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwriteLog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runs/logname/Network_properties.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Network: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Criterion: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/logname/Network_properties.txt'"
     ]
    }
   ],
   "source": [
    "logname=network+'_'+optimizer+'_'+criteria+'_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "writer = SummaryWriter('runs/'+logname)\n",
    "writeLog(logname)\n",
    "model_ft = ft.train_model(model_ft, criterion, optimizer_ft, lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                        use_gpu=use_gpu,num_epochs=50,batch_size=batch_size,num_log=200,multilabel=True,\n",
    "                          multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_ft,'./saved_models/'+logname)\n",
    "model_ft_backup=model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_ft=torch.load('./Obsolete/saved_models/resnet18_multi_88_real_7_15_17.mdl')\n",
    "model_ft=torch.load('./saved_models/resnet_real_ft5s_multisoft__1300_100epoch_July19  12:22:17')\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion=nn.MultiLabelSoftMarginLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters())\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "model_ft = model_ft.cuda()\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# --- to-be-optimized ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(next(iter(dset_loaders['train']))[0])\n",
    "#model_ft = models.alexnet(pretrained=True)\n",
    "\n",
    "model_ft=model_ft.cpu()\n",
    "model_ft.train(False)\n",
    "#new_classifier = nn.Sequential(*list(model_ft.classifier.children())[:-5])\n",
    "#model_ft.classifier = new_classifier\n",
    "#print(model_ft)\n",
    "model_params= list(model_ft.children())\n",
    "#model_params[1]=list(model_params[1])\n",
    "#print(model_params)\n",
    "new_ft = nn.Sequential(*list(model_params)[:-1])\n",
    "#print(new_ft)\n",
    "\n",
    "fvec_tr=np.zeros((20000,512))\n",
    "label_tr=np.zeros((20000))\n",
    "\n",
    "fvec_val=np.zeros((20000,512))\n",
    "label_val=np.zeros((20000))\n",
    "count=0;\n",
    "\n",
    "#inputs_t, classes_t = data=next(iter(dset_loaders['train']))\n",
    "#print(inputs_t.size())\n",
    "#fvec_t=new_ft(Variable(inputs_t))\n",
    "\n",
    "for data in dset_loaders['train']:\n",
    "    inputs_t, classes_t = data\n",
    "    fvec_t=new_ft(Variable(inputs_t))\n",
    "    #print(fvec_t)\n",
    "    fvec_t_cpu=fvec_t.cpu()\n",
    "    if(fvec_t_cpu.data.numpy().shape[0]==4):\n",
    "        fvec_tr[count:count+4,:]=fvec_t_cpu.data.numpy().reshape(4,-1)\n",
    "        label_tr[count:count+4]=classes_t.short().numpy()\n",
    "        count +=4\n",
    "fvec_tr=fvec_tr[:count,:]\n",
    "label_tr=label_tr[:count]\n",
    "\n",
    "\n",
    "count=0;\n",
    "for data in dset_loaders['val']:\n",
    "    inputs_t, classes_t = data\n",
    "    fvec_t=new_ft(Variable(inputs_t))\n",
    "    fvec_t_cpu=fvec_t.cpu()\n",
    "    if(fvec_t_cpu.data.numpy().shape[0]==4):\n",
    "        fvec_val[count:count+4,:]=fvec_t_cpu.data.numpy().reshape(4,-1)\n",
    "        label_val[count:count+4]=classes_t.short().numpy()\n",
    "        count +=4\n",
    "    \n",
    "fvec_val=fvec_val[:count,:]\n",
    "label_val=label_val[:count]\n",
    "\n",
    "'''\n",
    "print('The CNN model is')\n",
    "print(model_ft)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "#model_ft.classifier['6'] = nn.Linear(num_ftrs, 2)\n",
    "print(model_ft)\n",
    "'''\n",
    "\n",
    "print('Size of the input in training is '+str(fvec_tr.shape))\n",
    "print('Size of the labels in training is '+str(label_tr.shape))\n",
    "\n",
    "print('Size of the input in validation is '+str(fvec_val.shape))\n",
    "print('Size of the labels in validation is '+str(label_val.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fvec_val.shape)\n",
    "print(fvec_val[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#clf = SVC(C=1,kernel='linear',class_weight='balanced',decision_function_shape='ovo')\n",
    "clf=OneVsOneClassifier(LinearSVC(C=1e-5))\n",
    "clf.fit(fvec_tr,label_tr)\n",
    "label_pred=clf.predict(fvec_val)\n",
    "abs_err=np.abs(label_pred-label_val)\n",
    "cir1=np.mean(abs_err<=1)\n",
    "print(cir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c_val=np.power(2,np.arange(0.,8.,1.))\n",
    "g_val=np.power(2,np.arange(-10.,-5.,1.))\n",
    "\n",
    "#c_val=[128]\n",
    "#g_val=[np.power(2,-10.)]\n",
    "cir1_max=0.\n",
    "c_max=0\n",
    "g_max=0\n",
    "for c_1 in c_val:\n",
    "    for g in g_val:\n",
    "        \n",
    "        clf_svc = SVC(C=c_1,kernel='rbf',gamma=g,shrinking=False,class_weight='balanced')\n",
    "        clf=OneVsOneClassifier(clf_svc)\n",
    "        #print(label_tr)\n",
    "        clf.fit(fvec_tr,label_tr)\n",
    "\n",
    "        label_pred=clf.predict(fvec_val)\n",
    "        #print(label_pred)\n",
    "        abs_err=np.abs(label_pred-label_val)\n",
    "        #print(abs_err)\n",
    "        cir1=np.mean(abs_err<=1)\n",
    "        print('C = '+str(c_1),', g = ',str(g),', cir-1 = '+str(cir1))\n",
    "        if(cir1>cir1_max):\n",
    "            cir1_max=cir1\n",
    "            c_max=c_1\n",
    "            g_max=g\n",
    "            \n",
    "\n",
    "clf = SVC(C=c_max,kernel='rbf',gamma=g_max)#,class_weight='balanced')\n",
    "clf.fit(fvec_tr,label_tr)\n",
    "label_pred=clf.predict(fvec_val)\n",
    "abs_err=np.abs(label_pred-label_val)\n",
    "print('Max CIR-1 achieved is '+str(np.mean(abs_err<=1)) +' with c='+str(c_max),', g='+str(g_max))\n",
    "print('CIR-0 = '+str(np.mean(abs_err<=0)))\n",
    "print('CIR-2 = '+str(np.mean(abs_err<=2)))\n",
    "tr_pred=clf.predict(fvec_tr)\n",
    "abs_tr=np.abs(tr_pred-label_tr)\n",
    "print('Training CIR-1 is '+str(np.mean(abs_tr<=1)))\n",
    "#print(abs_err)\n",
    "#print(label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "#print('The CNN model is')\n",
    "#print(model_ft)\n",
    "#print(list(model_ft.children())[:-1])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "#setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "model_ft.fc = nn.Linear(num_ftrs, 9)\n",
    "#print(model_ft)\n",
    "\n",
    "#print(next(iter(dset_loaders['train']))[0])\n",
    "new_ft = nn.Sequential(*list(model_ft.children())[:-1])\n",
    "#dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=4,\n",
    "#                                               shuffle=True, num_workers=4)\n",
    "#                for x in ['train', 'val']}\n",
    "\n",
    "fvec_tr=np.zeros((X_tr.shape[0],512))\n",
    "for k in range(0,10,10):#(0,inputs.shape[0],10):\n",
    "    print(k)\n",
    "    fvec_now=new_ft(Variable(torch.from_numpy(X_tr[k:k+10,:,:,:])))\n",
    "    print(fvec_now.numpy())\n",
    "\n",
    "print(fvec)\n",
    "\n",
    "\n",
    "'''\n",
    "print('The CNN model is')\n",
    "print(model_ft)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "print('Number of features in the fine tune layer is '+str(num_ftrs))\n",
    "setattr(model_ft.classifier, '6', nn.Linear(num_ftrs, 9))\n",
    "#model_ft.classifier['6'] = nn.Linear(num_ftrs, 2)\n",
    "print(model_ft)\n",
    "'''\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet as fixed feature extractor\n",
    "----------------------------------\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set ``requires_grad == False`` to freeze the parameters so that the\n",
    "gradients are not computed in ``backward()``.\n",
    "\n",
    "You can read more about this in the documentation\n",
    "`here <http://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model_conv.fc.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 9)\n",
    "\n",
    "if use_gpu:\n",
    "    model_conv = model_conv.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_conv = ft.train_model(model_conv, criterion, optimizer_conv, ft.exp_lr_scheduler,dset_loaders,dset_sizes,writer,\n",
    "                        use_gpu=use_gpu,num_epochs=100,batch_size=32,num_log=1000,multilabel=False,multi_prob=False,\n",
    "                         lr_decay_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
